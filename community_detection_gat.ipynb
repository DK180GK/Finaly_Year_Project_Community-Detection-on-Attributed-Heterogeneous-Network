import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv
from torch_geometric.data import Data
from torch_geometric.utils import to_networkx
import numpy as np
import pandas as pd
import networkx as nx
from sklearn.cluster import SpectralClustering
from sklearn.metrics import normalized_mutual_info_score
import matplotlib.pyplot as plt
from scipy.sparse import csr_matrix
import warnings
warnings.filterwarnings('ignore')

class DataLoader:
    def __init__(self, data_dir="./"):
        self.data_dir = data_dir

    def load_authors(self):
        author_labels = pd.read_csv(f"{self.data_dir}reduced_author_label.txt",
                                     sep='\t',
                                     names=['author_id', 'label', 'author_name'],
                                     dtype={'author_id': int, 'label': int, 'author_name': str})
        return author_labels

    def load_papers(self):
        papers = pd.read_csv(f"{self.data_dir}reduced_paper.txt",
                              sep='\t',
                              names=['paper_id', 'paper_name'],
                              dtype={'paper_id': int, 'paper_name': str})

        paper_labels = pd.read_csv(f"{self.data_dir}reduced_paper_label.txt",
                                   sep='\t',
                                   names=['paper_id', 'label', 'paper_name'],
                                   dtype={'paper_id': int, 'label': int, 'paper_name': str})
        return papers, paper_labels

    def load_conferences(self):
        conferences = pd.read_csv(f"{self.data_dir}reduced_conf.txt",
                                   sep='\t',
                                   names=['conf_id', 'conf_name'],
                                   dtype={'conf_id': int, 'conf_name': str})

        conf_labels = pd.read_csv(f"{self.data_dir}reduced_conf_label.txt",
                                  sep='\t',
                                  header=None,
                                  names=['label', 'conf_name', 'extra'],
                                  usecols=[0, 1])

        conf_labels['conf_name'] = conf_labels['conf_name'].str.strip()
        conferences['conf_name'] = conferences['conf_name'].str.strip()

        conf_labels = pd.merge(conferences[['conf_id', 'conf_name']],
                               conf_labels,
                               on='conf_name',
                               how='inner')

        return conferences, conf_labels

    def load_relationships(self):
        paper_author = pd.read_csv(f"{self.data_dir}reduced_paper_author.txt",
                                   sep='\t',
                                   names=['paper_id', 'author_id'],
                                   dtype={'paper_id': int, 'author_id': int})

        paper_conf = pd.read_csv(f"{self.data_dir}reduced_paper_conf.txt",
                                 sep='\t',
                                 names=['paper_id', 'conf_id'],
                                 dtype={'paper_id': int, 'conf_id': int})
        return paper_author, paper_conf

class GraphConstructor:
    def __init__(self, data_loader):
        self.data_loader = data_loader
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def build_graph(self):
        author_labels = self.data_loader.load_authors()
        papers, paper_labels = self.data_loader.load_papers()
        conferences, conf_labels = self.data_loader.load_conferences()
        paper_author, paper_conf = self.data_loader.load_relationships()

        self.author_mapping = {id_: idx for idx, id_ in enumerate(author_labels['author_id'].unique())}
        self.paper_mapping = {id_: idx + len(self.author_mapping)
                               for idx, id_ in enumerate(papers['paper_id'].unique())}
        self.conf_mapping = {id_: idx + len(self.author_mapping) + len(self.paper_mapping)
                              for idx, id_ in enumerate(conferences['conf_id'].unique())}

        edges = []

        author_groups = author_labels.groupby('label')['author_id']
        for _, group in author_groups:
            group_list = sorted(group.tolist())
            for i in range(len(group_list)):
                for j in range(i + 1, len(group_list)):
                    edges.append((self.author_mapping[group_list[i]],
                                  self.author_mapping[group_list[j]]))

        paper_groups = paper_labels.groupby('label')['paper_id']
        for _, group in paper_groups:
            group_list = sorted(group.tolist())
            for i in range(len(group_list)):
                for j in range(i + 1, len(group_list)):
                    edges.append((self.paper_mapping[group_list[i]],
                                  self.paper_mapping[group_list[j]]))

        conf_groups = conf_labels.groupby('label')['conf_id']
        for _, group in conf_groups:
            group_list = sorted(group.tolist())
            for i in range(len(group_list)):
                for j in range(i + 1, len(group_list)):
                    edges.append((self.conf_mapping[group_list[i]],
                                  self.conf_mapping[group_list[j]]))

        edges_author_paper = [(self.author_mapping[aid], self.paper_mapping[pid])
                               for aid, pid in zip(paper_author['author_id'], paper_author['paper_id'])]

        edges_paper_conf = [(self.paper_mapping[pid], self.conf_mapping[cid])
                             for pid, cid in zip(paper_conf['paper_id'], paper_conf['conf_id'])]

        # New: Adding author-conference connections
        edges_author_conf = [(self.author_mapping[aid], self.conf_mapping[cid])
                              for aid, cid in zip(paper_author['author_id'], paper_conf['conf_id'])
                              if aid in self.author_mapping and cid in self.conf_mapping]

        edges.extend(edges_author_paper)
        edges.extend(edges_paper_conf)
        edges.extend(edges_author_conf)  # Add new edges here
        edges.extend([(j, i) for i, j in edges])

        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()

        num_nodes = len(self.author_mapping) + len(self.paper_mapping) + len(self.conf_mapping)
        x = torch.eye(num_nodes, device=self.device)
        labels = torch.zeros(num_nodes, dtype=torch.long, device=self.device)

        for entity_type, mapping, df in [
            ('author', self.author_mapping, author_labels),
            ('paper', self.paper_mapping, paper_labels),
            ('conf', self.conf_mapping, conf_labels)
        ]:
            indices = torch.tensor([mapping[id_] for id_ in df['conf_id' if entity_type == 'conf' else 'paper_id' if entity_type == 'paper' else 'author_id']],
                                   device=self.device)
            labels[indices] = torch.tensor(df['label'].values, dtype=torch.long, device=self.device)

        return Data(x=x, edge_index=edge_index, y=labels)

class GATModel(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, heads=4):
        super().__init__()
        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)
        self.conv2 = GATConv(hidden_channels * heads, out_channels)

    def forward(self, x, edge_index):
        x = F.dropout(x, p=0.3, training=self.training)
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        x = F.dropout(x, p=0.3, training=self.training)
        x = self.conv2(x, edge_index)
        return x

class ModelTrainer:
    def __init__(self, model, data, num_epochs=100, lr=0.01):
        self.model = model
        self.data = data
        self.num_epochs = num_epochs
        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    def train(self):
        self.model.train()
        for epoch in range(self.num_epochs):
            self.optimizer.zero_grad()
            out = self.model(self.data.x, self.data.edge_index)
            loss = F.cross_entropy(out, self.data.y)
            loss.backward()
            self.optimizer.step()

            if (epoch + 1) % 10 == 0:
                print(f'Epoch {epoch+1}/{self.num_epochs}, Loss: {loss.item():.4f}')

class ClusterEvaluator:
    def __init__(self, embeddings, graph, n_clusters=4):
        self.embeddings = embeddings
        self.graph = graph
        self.n_clusters = n_clusters

    def perform_clustering(self):
        clustering = SpectralClustering(n_clusters=self.n_clusters,
                                        affinity='nearest_neighbors')
        self.clusters = clustering.fit_predict(self.embeddings.detach().cpu().numpy())
        return self.clusters

    def compute_metrics(self):
        nx_graph = to_networkx(self.graph)
        communities = {i: np.where(self.clusters == i)[0] for i in range(self.n_clusters)}

        modularity = nx.community.modularity(nx_graph, communities.values())
        nmi = normalized_mutual_info_score(self.graph.y.cpu().numpy(), self.clusters)

        conductances = []
        densities = []

        for cluster_nodes in communities.values():
            cluster_nodes = list(cluster_nodes)

            if not cluster_nodes or len(cluster_nodes) == nx_graph.number_of_nodes():
                continue

            other_nodes = list(set(nx_graph.nodes()) - set(cluster_nodes))

            if other_nodes:
                cut_size = nx.cut_size(nx_graph, cluster_nodes, other_nodes)
                cluster_volume = sum(dict(nx_graph.degree(cluster_nodes)).values())
                remaining_volume = sum(dict(nx_graph.degree()).values()) - cluster_volume
                conductance = cut_size / min(cluster_volume, remaining_volume) if min(cluster_volume, remaining_volume) > 0 else 1.0
                conductances.append(conductance)

            subgraph = nx_graph.subgraph(cluster_nodes)
            n = subgraph.number_of_nodes()
            m = subgraph.number_of_edges()
            density = (2.0 * m) / (n * (n - 1)) if n > 1 else 0.0
            densities.append(density)

        avg_conductance = np.mean(conductances) if conductances else 0.0
        avg_density = np.mean(densities) if densities else 0.0

        return {
            'modularity': modularity,
            'nmi': nmi,
            'conductance': avg_conductance,
            'density': avg_density
        }

    def visualize_clusters(self):
        plt.figure(figsize=(12, 8))
        nx_graph = to_networkx(self.graph)
        pos = nx.spring_layout(nx_graph)

        for i in range(self.n_clusters):
            nodes = np.where(self.clusters == i)[0]
            nx.draw_networkx_nodes(nx_graph, pos, nodelist=nodes,
                                   node_color=f'C{i}', node_size=50, label=f'Cluster {i}')

        nx.draw_networkx_edges(nx_graph, pos, alpha=0.2)
        plt.legend()
        plt.title('Academic Network Communities')
        plt.show()

def main():
    data_loader = DataLoader()
    graph_constructor = GraphConstructor(data_loader)
    data = graph_constructor.build_graph()

    print(f"Number of nodes: {data.num_nodes}")
    print(f"Number of edges: {data.edge_index.size(1)}")
    print(f"Number of node features: {data.num_features}")
    print(f"Number of classes: {data.y.max().item() + 1}")

    model = GATModel(in_channels=data.num_features,
                     hidden_channels=64,
                     out_channels=64).to(data.x.device)

    trainer = ModelTrainer(model, data)
    trainer.train()

    model.eval()
    with torch.no_grad():
        embeddings = model(data.x, data.edge_index)

    evaluator = ClusterEvaluator(embeddings, data)
    clusters = evaluator.perform_clustering()
    metrics = evaluator.compute_metrics()

    print("\nClustering Metrics:")
    for metric, value in metrics.items():
        print(f"{metric}: {value:.4f}")

    evaluator.visualize_clusters()

if __name__ == "__main__":
    main()
